---
title: "Meta-regression"
author: "Audrey Beliveau, Justin Slater"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: references.bib
vignette: >
  %\VignetteIndexEntry{Meta-regression}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

``` {r}
library(BUGSnet)
library(gridExtra)
data(afib)
dataprep <- data.prep(arm.data = afib,
                      varname.t = "treatment",
                      varname.s = "study")
```

## Feasibility Assessment

### Network Plot


```{r, net.plot, echo=TRUE, fig.width=5, fig.height=5}
 net.plot(dataprep, node.scale = 1, 
          edge.scale=1,
          label.offset1 = 2)
```


### Generate Network Characteristics via `net.tab()`

```{r, results = "hide"}
network.char <- net.tab(data = dataprep,
                        outcome = "events",
                        N = "sampleSize",
                        type.outcome = "binomial")
```

#### Network Characteristics
`network.char$network` generates characteristics about the network, such as connectedness, number of treatments in the network, and in the case of a binomial outcome, the number of events in the network.
```{r, echo = FALSE}
knitr::kable(network.char$network)
```

#### Intervention Characteristics
`network.char$intervention` generates outcome and sample size data broken down by treatment.

```{r, echo=FALSE}
knitr::kable(network.char$intervention)
```

#### Comparison Characteristics
`network.char$comparison` generates outcome and sample size data broken down by treatment **comparison**.
```{r, echo=FALSE}
knitr::kable(network.char$comparison)
```

## Main analysis
`nma.model()` creates BUGS code and that will be put into `nma.run()` and analysed through JAGS [@JAGS]. The `reference` parameter indicates the name of the treatment that will be seen as the 'referent' comparator, this is often a placebo of some sort. In our case, it is treatment 02. Since our outcome is dichotomous, and we are not interested in event rates, we are using the "binomial" family. In our case, we want to compare odds ratios, so we are using the $logit$ link.
```{r}
fixed_effects_model <- nma.model(data=dataprep,
                                  outcome="events",
                                  N="sampleSize",
                                  reference="02",
                                  family="binomial",
                                  link="logit",
                                  effects="fixed",
                                  covariate="stroke",
                                  prior.beta="EXCHANGEABLE")

random_effects_model <- nma.model(data=dataprep,
                                  outcome="events",
                                  N="sampleSize",
                                  reference="02",
                                  family="binomial",
                                  link="logit",
                                  effects="random",
                                  covariate="stroke",
                                  prior.beta="EXCHANGEABLE")

```

If you want to review the BUGS code, you can review it by outputting `cat(random_effects_model$bugs)`. 

The next step is to run the NMA model using `nma.run()`. Since we are working in a Bayesian framework, we need to specify the number of adaptations, burn-ins, and iterations. A description of Bayesian MCMC is omitted here, we direct the reader to any introductory text on Bayesian Modelling [@lunn2012bugs].

```{r, results = "hide"}
fixed_effects_results <- nma.run(fixed_effects_model,
                           n.adapt=1000,
                           n.burnin=1000,
                           n.iter=5000)

random_effects_results <- nma.run(random_effects_model,
                           n.adapt=1000,
                           n.burnin=1000,
                           n.iter=5000)

```

# Model Choice
Compare fixed vs random effects models by comparing leverage plots and the DIC
```{r, fig.width=7, fig.height=4, results = "hide"}
fe <- nma.fit(fixed_effects_results, main = "Fixed Effects Model" )
re <- nma.fit(random_effects_results, main= "Random Effects Model")
grid.arrange(fe$plot, re$plot, ncol = 2)
```

The random effects model seems to be more appropriate here due to a lower DIC, and slightly better leverage plot.

<!-- Next, we will assess consistency in the network by fitting an inconsistency fixed effects model and comparing it to our -->
<!-- consistency fixed effects model. If our inconsistency model shows a better fit than the consistency model, then  -->
<!-- it is likely that there is inconsistency in the network. -->

<!-- ##Check inconsistency -->
<!-- ```{r, results = "hide", fig.show = 'hide',  fig.width=8, fig.height = 8} -->
<!-- re_inconsistency_model <- nma.model(data=dataprep, -->
<!--                                   outcome="events", -->
<!--                                   N="sampleSize", -->
<!--                                   reference="02", -->
<!--                                   family="binomial", -->
<!--                                   link="logit", -->
<!--                                   effects="random", -->
<!--                                   type="inconsistency", -->
<!--                                   covariate="stroke", -->
<!--                                   prior.beta="EXCHANGEABLE") -->

<!-- re_inconsistency_results <- nma.run(re_inconsistency_model, -->
<!--                                          n.adapt=1000, -->
<!--                                          n.burnin=1000, -->
<!--                                          n.iter=10000) -->

<!-- ``` -->

<!-- Rainbow plots and DIC calculations can highlight outliers and can compare model fits between the two models.  -->
<!-- ```{r, fig.width=7, fig.height=4, results = "hide"} -->
<!-- par(mfrow = c(1,2)) -->
<!-- re_model_fit <- nma.fit(random_effects_results, main = "Consistency Model" ) -->
<!-- inconsist_model_fit <- nma.fit(re_inconsistency_results, main= "Consistency Model") -->
<!-- ``` -->
<!-- The leverage plots and DIC show that the consistency and inconsistency models are very similar, suggesting that -->
<!-- a consistency model is satisfactory. -->

<!-- A plot of the \code{pmdev} of both models against each other can highlight descrepancies between the two models. -->
<!-- ```{r, fig.height=6, fig.width = 6} -->
<!-- nma.compare(re_model_fit, inconsist_model_fit) -->
<!-- ``` -->

<!-- Seeing as that all the points lie close to the $y=x$ line, there is very little evidence of inconsistency. -->

## Results

### Regression plot
We can see the effect of the covariate on the log odds ratio on the following plot
```{r, echo=TRUE, fig.width=7, fig.height=4}
nma.regplot(random_effects_results)
```

The lines are not parallel because we specified EXCHANGEABLE priors on the regression coefficients.

Let's looks at the relative treatment differences and ranking of treaments when the proportion of patients with a prior stroke is equal to 0.1.

### Sucra Plot
```{r, echo=TRUE, fig.width=7, fig.height=4, dpi = 95}
sucra.out <- nma.rank(random_effects_results, largerbetter=FALSE, cov.value=0.1, sucra.palette= "Set1")
sucra.out$sucraplot
```

### League Plot
```{r, echo=TRUE, fig.width=15, fig.height=10}
league.out <- nma.league(random_effects_results, 
                             central.tdcy = "median",
                             order = as.vector(t(dataprep$treatments)),
                             cov.value=0.1, 
                             log.scale = FALSE)
league.out$heatplot
```

### Forest Plot
```{r, echo=TRUE, fig.width=10, fig.height=4}
nma.forest(random_effects_results, 
           comparator="02", 
           central.tdcy = "median",
           cov.value=0.1,
           x.trans="log")
```



# References
